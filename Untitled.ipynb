{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from PyQt5 import QtWidgets\n",
    "from PyQt5.QtWidgets import QApplication,QMainWindow\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "document_matrix = {}\n",
    "\n",
    "def create_stoplist(): #creating stopword list \n",
    "    stop_list_f = open(\"Stopword-list.txt\",\"r\")\n",
    "    stop_list = stop_list_f.read()\n",
    "    stop_list.replace(\" \",\"\")\n",
    "    stopword_list = stop_list.split(\"\\n\")\n",
    "    return stopword_list\n",
    "\n",
    "def create_wordlist(docID): #creating list of all words document\n",
    "    f = open(str(docID)+\".txt\", \"r\",encoding=\"utf-8\")\n",
    "    f_read=f.read()\n",
    "#     print(f_read)\n",
    "    f_read=f_read.lower() #lower case complete document \n",
    "    f_read = f_read.replace(\"\\n\",\" \") #removing new line character\n",
    "    f_read = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,“”’———‘*1234567890]\",\"\", f_read) #removing punctuations\n",
    "    f_list = f_read.split(\" \") \n",
    "    f_list=[x for x in f_list if x] #creating list of words\n",
    "    # print(f_list)\n",
    "    # print(len(f_list))\n",
    "    words = [x for x in f_list if x not in stopword_list] #removing stopwords from the list of words\n",
    "#     words = [lemmatizer.lemmatize(x) for x in words]\n",
    "    return words\n",
    "#     print(words)\n",
    "#     print(len(words))\n",
    "\n",
    "\n",
    "def create_term_frequency(words,docID):\n",
    "#     print(words)\n",
    "    for pos,term in enumerate(words):\n",
    "#         print(words[word])\n",
    "        if term not in term_frequency:\n",
    "            term_frequency[term] = []\n",
    "            term_frequency[term].append(1)\n",
    "            term_frequency[term].append({})\n",
    "            term_frequency[term][1][docID] = 1\n",
    "        else:\n",
    "#             print(term_frequency)\n",
    "            if docID in term_frequency[term][1]:\n",
    "                term_frequency[term][1][docID] = term_frequency[term][1][docID] + 1\n",
    "            else:\n",
    "                term_frequency[term][1][docID] = 1\n",
    "                term_frequency[term][0] = term_frequency[term][0] + 1\n",
    "    \n",
    "    sorted_index = sorted(term_frequency.items(), key = lambda kv: kv[0])\n",
    "    sorted_index = dict(sorted_index)\n",
    "    return sorted_index\n",
    "#         else:\n",
    "#             print(1)\n",
    "\n",
    "def create_matrix(words,words_dictionary,docID):\n",
    "    for i in range(0,len(words_dictionary)):\n",
    "        if docID not in document_matrix:\n",
    "            document_matrix[docID] = []\n",
    "        document_matrix[docID].append(0)\n",
    "#     print(document_matrix)\n",
    "    for i in range(0,len(words)):\n",
    "        index = words_dictionary.index(words[i])\n",
    "        document_matrix[docID][index] = 1\n",
    "        \n",
    "    return document_matrix\n",
    "\n",
    "\n",
    "def create_idf_matrix(words_dictionary,term_frequency):\n",
    "    idf_values = {}\n",
    "#     print(term_frequency[words_dictionary[0]][0])\n",
    "    for i in range(0,len(words_dictionary)):\n",
    "        temp = math.log10(50/term_frequency[words_dictionary[i]][0])\n",
    "        idf_values[words_dictionary[i]] = temp\n",
    "    return idf_values\n",
    "\n",
    "\n",
    "def create_tf_idf_matrix(words,words_dictionary,docID,idf_values,term_frequency):\n",
    "#     for i in range(0,len(words_dictionary)):\n",
    "    if docID not in document_matrix:\n",
    "        document_matrix[docID] = []\n",
    "    document_matrix[docID] = [0] * len(words_dictionary)\n",
    "    for i in range(0,len(words)):\n",
    "        index = words_dictionary.index(words[i])\n",
    "        temp = idf_values[words[i]] * term_frequency[words[i]][1][docID]\n",
    "        document_matrix[docID][index] = temp\n",
    "    return document_matrix\n",
    "#     j = 0\n",
    "#     for i in range(0,len(words_dictionary)):\n",
    "#         if words[j] != words_dictionary[i]:\n",
    "\n",
    "\n",
    "def find_magnitudes_of_documents(tf_idf_matrix):\n",
    "#     keys = list(tf_idf_matrix.keys())\n",
    "#     print(keys)\n",
    "    magnitude = {}\n",
    "    for i in range(1,51):\n",
    "#         print(i)\n",
    "        squared_numbers = [number ** 2 for number in tf_idf_matrix[i]]\n",
    "#         print(squared_numbers)\n",
    "        total = sum(squared_numbers)\n",
    "#         print(total)\n",
    "        magnitude[i] = math.sqrt(total)\n",
    "#         print(magnitude[i])\n",
    "    return magnitude\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9208187539523752\n"
     ]
    }
   ],
   "source": [
    "words_dictionary = []\n",
    "stopword_list = []\n",
    "words = []\n",
    "stopword_list = create_stoplist()\n",
    "term_frequency = {}\n",
    "tf_idf_matrix = {}\n",
    "\n",
    "document_frequency = {}\n",
    "\n",
    "for docID in range (1,51):\n",
    "    words = create_wordlist(docID)\n",
    "    words.sort()\n",
    "    term_frequency = create_term_frequency(words,docID)\n",
    "\n",
    "words_dictionary = list(term_frequency.keys()) ##creating list of all terms in document\n",
    "# print(words_dictionary.index('across'))\n",
    "\n",
    "idf_values = create_idf_matrix(words_dictionary,term_frequency)\n",
    "print(idf_values['due'])\n",
    "\n",
    "\n",
    "for docID in range (1,51):\n",
    "    tf_idf_matrix = {}\n",
    "    words = create_wordlist(docID)\n",
    "    words.sort()\n",
    "    tf_idf_matrix = create_tf_idf_matrix(words,words_dictionary,docID,idf_values,term_frequency)\n",
    "\n",
    "\n",
    "\n",
    "# print(len(tf_idf_matrix))\n",
    "\n",
    "# for i in range(0,len(tf_idf_matrix[50])):\n",
    "#     if tf_idf_matrix[50][i] != 0:\n",
    "#         print(i)\n",
    "#         break\n",
    "        \n",
    "# print(document_matrix[1])\n",
    "#     words2.append([x for x in words if x in words])\n",
    "# print(term_frequency[\"due\"])\n",
    "# print(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 146.98226074165208, 2: 351.20197224032097, 3: 65.18483260178579, 4: 35.80312475269331, 5: 40.15074389978858, 6: 52.96638267268419, 7: 54.79567487670235, 8: 60.599078413702145, 9: 41.57423248309349, 10: 57.86466314974281, 11: 60.3445225867355, 12: 47.00384915703946, 13: 38.62518592279566, 14: 45.583939586802884, 15: 54.93246542381947, 16: 46.38896480496068, 17: 63.200419579595405, 18: 25.031101178167084, 19: 42.45653070503877, 20: 64.61778477360915, 21: 60.22836372413668, 22: 57.43752628711682, 23: 105.1920416651297, 24: 39.152100148715164, 25: 162.65257969405275, 26: 71.155467580755, 27: 41.675686500376756, 28: 55.254074343715075, 29: 54.55087578953154, 30: 35.07231699987089, 31: 55.571556787761715, 32: 37.84958066237195, 33: 45.25621698833758, 34: 49.66800510523361, 35: 49.82768837455404, 36: 44.60506989303298, 37: 44.284682263650744, 38: 45.866425706872036, 39: 47.854253700951325, 40: 26.004337028712566, 41: 45.43254307704153, 42: 36.97036337796894, 43: 49.15988779318244, 44: 38.127082172794395, 45: 31.93553026236736, 46: 23.111585810117354, 47: 29.591901628373783, 48: 25.74911058001826, 49: 44.338431120425135, 50: 6.912506551216887}\n"
     ]
    }
   ],
   "source": [
    "magnitudes = {}\n",
    "magnitudes = find_magnitudes_of_documents(tf_idf_matrix)\n",
    "print(magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q_term_frequency(new_query):\n",
    "    query_term_frequency = {}\n",
    "    for pos,term in enumerate(new_query):\n",
    "        if term not in query_term_frequency:\n",
    "            query_term_frequency[term] = 1\n",
    "        else:\n",
    "            query_term_frequency[term] = query_term_frequency[term] + 1\n",
    "    query_sorted_index = sorted(query_term_frequency.items(), key = lambda kv: kv[0])\n",
    "    query_sorted_index = dict(query_sorted_index)\n",
    "    return query_sorted_index\n",
    "\n",
    "def create_q_tf_idf_matrix(query_term_frequency,idf_values):\n",
    "    keys = list(query_term_frequency.keys()) \n",
    "    query_tf_idf_matrix = {}\n",
    "    for i in range(0,len(query_term_frequency)):\n",
    "        query_tf_idf_matrix[keys[i]] = query_term_frequency[keys[i]] * idf_values[keys[i]]\n",
    "#         query_tf_idf_matrix[keys[i]] = query_term_frequency[keys[i]] * 1\n",
    "    return query_tf_idf_matrix\n",
    "\n",
    "def q_magnitude(query_tf_idf_matrix):\n",
    "    keys = list(query_tf_idf_matrix.keys())\n",
    "    total = 0\n",
    "    for i in range (0,len(query_tf_idf_matrix)):\n",
    "        temp = query_tf_idf_matrix[keys[i]]**2\n",
    "        total = total + temp\n",
    "    return (math.sqrt(total))\n",
    "\n",
    "\n",
    "def get_posting_list(new_query):\n",
    "    temp = []\n",
    "    for i in range(0,len(new_query)):\n",
    "        temp.extend(list(term_frequency[new_query[i]][1].keys()))\n",
    "#         print(temp)\n",
    "    temp = list(dict.fromkeys(temp))\n",
    "    return temp \n",
    "\n",
    "\n",
    "def get_term_index_from_vocabulary(new_query,words_dictionary):\n",
    "    temp = []\n",
    "    for i in range(0,len(new_query)):\n",
    "        temp.append(words_dictionary.index(new_query[i]))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query):\n",
    "    new_query = query.split()\n",
    "    flag = 0\n",
    "    for i in range(0,len(new_query)):\n",
    "        if new_query[i] in words_dictionary:\n",
    "            flag=1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        return 0\n",
    "#     print(new_query)\n",
    "    query_term_frequency = q_term_frequency(new_query)\n",
    "#     print(query_term_frequency)\n",
    "    query_tf_idf_matrix = create_q_tf_idf_matrix(query_term_frequency,idf_values)\n",
    "#     print(query_tf_idf_matrix['across'])\n",
    "#     print(query_tf_idf_matrix['adventure'])\n",
    "    query_magnitude = q_magnitude(query_tf_idf_matrix)\n",
    "#     print(query_magnitude)\n",
    "    documents = get_posting_list(new_query)\n",
    "    term_index = get_term_index_from_vocabulary(new_query,words_dictionary)\n",
    "#     print(term_index)\n",
    "    cosine_scores = {}\n",
    "    for l in range(0,len(documents)):\n",
    "        total = 0\n",
    "        temp = 0\n",
    "        for m in range(0,len(term_index)):\n",
    "            temp = tf_idf_matrix[documents[l]][term_index[m]] * query_tf_idf_matrix[new_query[m]]\n",
    "            total = total + temp\n",
    "#         print (\"document\",documents[l],\"total\",total)\n",
    "        cosine_temp = total/(magnitudes[documents[l]] * query_magnitude)\n",
    "        cosine_scores[documents[l]] = cosine_temp\n",
    "#     cosine_scores = sorted(cosine_scores.items(), key = lambda kv: kv[1])\n",
    "    {k: v for k, v in sorted(cosine_scores.items(), key=lambda item: item[1], reverse = True)}\n",
    "#     print(cosine_scores)\n",
    "    return cosine_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Query: ahsan\n",
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-426-303b5469989f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msorted_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your Query: \")\n",
    "result = []\n",
    "result = query_processing(query)\n",
    "print(result)\n",
    "\n",
    "sorted_result = dict(sorted(result.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print(sorted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(words_dictionary[138]),86\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8593662748651818"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix[1][86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, {1: 6, 2: 11, 10: 1, 12: 1, 23: 1, 27: 6, 34: 6}]\n",
      "0.585026652029182\n"
     ]
    }
   ],
   "source": [
    "print(term_frequency['assistant'])\n",
    "# print(term_frequency['adventure'])\n",
    "# print(idf_values['across'])\n",
    "print(idf_values['horse'])\n",
    "# print(magnitudes[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
